model_name_or_path: "EleutherAI/llemma_7b"
data: gsm8k
data_cache_dir: /weka/proj-llemma-instruct/.cache
trainer:
  mp: p=f32,c=bfloat16
  wandb:
    name: "test"
    project: "levanter"
    tags: ["gsm8k", "lora", "llama2"]
  num_train_steps: 100  # 64 * 550 = 35200, which is a bit more than 4 epochs
  train_batch_size: 2

  # if using model parallelism, this is useful:
  tensor_parallel_axes: ["mlp", "heads"]
optimizer:
  # values in qlora paper
  learning_rate: 2e-4
  weight_decay: 0.0
  lr_schedule: "constant"
lora:
  # These are the defaults, but just so you can see them
  r: 8  # rank of LoRA transform
  alpha: 8.0  # scaling factor for LoRA transform
  dropout: 0.0  # dropout probability for LoRA layers
hf_save_steps: 100
